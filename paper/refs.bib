@inproceedings{Chen2014,
author = {Chen, Tianqi and Fox, Emily and Guestrin, Carlos},
booktitle = {Proceedings of The 31st International Conference on Machine Learning},
file = {:home/leonard/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Fox, Guestrin - 2014 - Stochastic Gradient Hamiltonian Monte Carlo(2).pdf:pdf},
keywords = {SGMCMC},
mendeley-tags = {SGMCMC},
pages = {1683--1691},
title = {{Stochastic Gradient Hamiltonian Monte Carlo}},
url = {http://jmlr.org/proceedings/papers/v32/cheni14},
year = {2014}
}
@inproceedings{Ding2014,
author = {Ding, Nan and Fang, Youhan and Babbush, Ryan and Chen, Changyou and Skeel, Robert D. and Neven, Hartmut},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/leonard/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2014 - Bayesian Sampling Using Stochastic Gradient Thermostats.pdf:pdf},
keywords = {SGMCMC},
mendeley-tags = {SGMCMC},
pages = {3203--3211},
title = {{Bayesian Sampling Using Stochastic Gradient Thermostats}},
url = {http://papers.nips.cc/paper/5592-bayesian-sampling-using-stochastic-gradient-thermostats},
year = {2014}
}
@article{Ma2015,
abstract = {Many recent Markov chain Monte Carlo (MCMC) samplers leverage stochastic dynamics with state adaptation to define a Markov transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable MCMC algorithms via data subsampling and using stochastic gradients in the stochastic dynamic simulations. However, such stochastic gradient MCMC methods have used simple stochastic dynamics, or required significant physical intuition to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We demonstrate the utility of our recipe by trivially "reinventing" previously proposed stochastic gradient MCMC samplers, and in proposing a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.},
archivePrefix = {arXiv},
arxivId = {1506.04696},
author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
eprint = {1506.04696},
file = {:home/leonard/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ma, Chen, Fox - 2015 - A Complete Recipe for Stochastic Gradient MCMC.pdf:pdf},
keywords = {SGMCMC},
mendeley-tags = {SGMCMC},
month = {jun},
title = {{A Complete Recipe for Stochastic Gradient MCMC}},
url = {http://arxiv.org/abs/1506.04696},
year = {2015}
}
@article{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {1206.1901},
author = {Neal, Radford M.},
doi = {doi:10.1201/b10905-6},
eprint = {1206.1901},
file = {:home/leonard/Dropbox/Papers/MCMCHandbookHMC.pdf:pdf},
isbn = {9781420079418},
issn = {{\textless}null{\textgreater}},
journal = {Handbook of Markov Chain Monte Carlo},
keywords = {hamiltonian dynamics,mcmc},
pages = {113--162},
title = {{MCMC using Hamiltonian dynamics}},
year = {2011}
}
@inproceedings{Welling2011,
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an in-built protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
author = {Welling, M. and Teh, Y.-W.},
booktitle = {Proceedings of the 28th International Conference on Machine Learning},
editor = {Getoor, L. and Scheffer, T.},
file = {:home/leonard/Dropbox/Papers/SGLD{\_}icmlpaper.pdf:pdf},
keywords = {Bayesian learning,ICML,machine learning,online learning},
pages = {681--688},
publisher = {Omnipress},
title = {{Bayesian Learning via Stochastic Gradient Langevin Dynamics}},
year = {2011}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/leonard/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@inproceedings{Shang2015,
author = {Shang, Xiaocheng and Zhu, Zhanxing and Leimkuhler, Benedict and Storkey, Amos J.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/leonard/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shang et al. - 2015 - Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling.pdf:pdf},
pages = {37--45},
title = {{Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling}},
url = {https://papers.nips.cc/paper/5978-covariance-controlled-adaptive-langevin-thermostat-for-large-scale-bayesian-sampling},
year = {2015}
}
@article{Leimkuhler2016,
abstract = {We study numerical methods for sampling probability measures in high dimension where the underlying model is only approximately identified with a gradient system. Extended stochastic dynamical methods are discussed which have application to multiscale models, nonequilibrium molecular dynamics, and Bayesian sampling techniques arising in emerging machine learning applications. In addition to providing a more comprehensive discussion of the foundations of these methods, we propose a new numerical method for the adaptive Langevin/stochastic gradient Nos{\'{e}}--Hoover thermostat that achieves a dramatic improvement in numerical efficiency over the most popular stochastic gradient methods reported in the literature. We also demonstrate that the newly established method inherits a superconvergence property (fourth order convergence to the invariant measure for configurational quantities) recently demonstrated in the setting of Langevin dynamics. Our findings are verified by numerical experiments.},
author = {Leimkuhler, Benedict and Shang, Xiaocheng},
doi = {10.1137/15M102318X},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {37M25,60H35,65C30,Bayesian sampling,adaptive thermostat,ergodicity,invariant measure,machine learning,stochastic differential equations},
language = {en},
month = {mar},
number = {2},
pages = {A712--A736},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Adaptive Thermostats for Noisy Gradient Systems}},
url = {http://epubs.siam.org/doi/10.1137/15M102318X},
volume = {38},
year = {2016}
}
